{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "aheK7zB3aglj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtXh6Ylq1zvq",
        "outputId": "c3fd4cba-3ce4-4d8c-eee8-95e7f46fa6bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 31 08:30:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell: basic helpers\n",
        "!pip install onnx onnxruntime onnxsim pycuda\n",
        "# NVIDIA Python index + tensorrt (often needed)\n",
        "!pip install nvidia-pyindex\n",
        "!pip install --upgrade nvidia-tensorrt\n",
        "# Optional: torch-tensorrt to compile PyTorch models directly\n",
        "!pip install torch-tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rugpddf2sXX",
        "outputId": "e471a210-212b-4a1b-fda3-3c11ed438b2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting onnxsim\n",
            "  Downloading onnxsim-0.4.36.tar.gz (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from onnxsim) (13.9.4)\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.2.3-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pycuda) (4.3.8)\n",
            "Requirement already satisfied: mako in /usr/lib/python3/dist-packages (from pycuda) (1.1.3)\n",
            "Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n",
            "  Downloading siphash24-1.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->onnxsim) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->onnxsim) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n",
            "Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytools-2025.2.3-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading siphash24-1.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.9/102.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: onnxsim, pycuda\n",
            "  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for onnxsim: filename=onnxsim-0.4.36-cp312-cp312-linux_x86_64.whl size=2200370 sha256=b4729bc52cb2f44bb6275818f5138fffeb383244810fe1f9fd400b4928453332\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/5d/cc/db1350d9fabfe7f8442b5d97aff2ff543fc253277f71a6508f\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1.1-cp312-cp312-linux_x86_64.whl size=659048 sha256=5a3809e3bd36e36ed50c2f0171bcc150bee6874c0f488586cfb7155f83258fc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/b9/56/7c378438d69d29106999eda1e46115a1c4794308a201c77e6b\n",
            "Successfully built onnxsim pycuda\n",
            "Installing collected packages: siphash24, humanfriendly, pytools, onnx, coloredlogs, pycuda, onnxsim, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.0 onnxruntime-1.22.1 onnxsim-0.4.36 pycuda-2025.1.1 pytools-2025.2.3 siphash24-1.7\n",
            "Collecting nvidia-pyindex\n",
            "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8419 sha256=a9d9fcc30d65f036872c9ccada4ebd1f58594ad12c91dbf5d3093cf5e808c142\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/2d/7f/d86cb060a9c51fb933aa4fe0d2f73ffe8df2bd0b58d3d2bba4\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.9\n",
            "Collecting nvidia-tensorrt\n",
            "  Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl.metadata (596 bytes)\n",
            "Collecting tensorrt (from nvidia-tensorrt)\n",
            "  Downloading tensorrt-10.13.2.6.tar.gz (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu13==10.13.2.6 (from tensorrt->nvidia-tensorrt)\n",
            "  Downloading tensorrt_cu13-10.13.2.6.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu13_libs==10.13.2.6 (from tensorrt_cu13==10.13.2.6->tensorrt->nvidia-tensorrt)\n",
            "  Downloading tensorrt_cu13_libs-10.13.2.6.tar.gz (704 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu13_bindings==10.13.2.6 (from tensorrt_cu13==10.13.2.6->tensorrt->nvidia-tensorrt)\n",
            "  Downloading tensorrt_cu13_bindings-10.13.2.6-cp312-none-manylinux_2_28_x86_64.whl.metadata (606 bytes)\n",
            "Collecting nvidia-cuda-runtime-cu13 (from tensorrt_cu13_libs==10.13.2.6->tensorrt_cu13==10.13.2.6->tensorrt->nvidia-tensorrt)\n",
            "  Downloading nvidia_cuda_runtime_cu13-0.0.0a0-py2.py3-none-any.whl.metadata (225 bytes)\n",
            "Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl (17 kB)\n",
            "Downloading tensorrt_cu13_bindings-10.13.2.6-cp312-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu13-0.0.0a0-py2.py3-none-any.whl (1.2 kB)\n",
            "Building wheels for collected packages: tensorrt, tensorrt_cu13, tensorrt_cu13_libs\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.13.2.6-py2.py3-none-any.whl size=46436 sha256=272af62902bee59be69dd674c37ca05dcbbe03e99728bf9ec551a1c2b318e9bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/22/e7/4d28e4907a118d0a014adfec88205918792bb11cd061646091\n",
            "  Building wheel for tensorrt_cu13 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu13: filename=tensorrt_cu13-10.13.2.6-py2.py3-none-any.whl size=17437 sha256=9cfd14917ba66461f87586e36efc15d64def896fd51a4f1266b1edbdfa03788f\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/a4/19/4b06ee6d941601c0795594f260be07c42532c8a4a24cf6a1ff\n",
            "  Building wheel for tensorrt_cu13_libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu13_libs: filename=tensorrt_cu13_libs-10.13.2.6-py2.py3-none-manylinux_2_28_x86_64.whl size=2740427176 sha256=044468b0705728c0c8f0e95325ba881ff3e23bd2eebce0b1e628953ff90bbb5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/38/12/357ce01af52af73540a307d0f500629f7683148385297f336b\n",
            "Successfully built tensorrt tensorrt_cu13 tensorrt_cu13_libs\n",
            "Installing collected packages: tensorrt_cu13_bindings, nvidia-cuda-runtime-cu13, tensorrt_cu13_libs, tensorrt_cu13, tensorrt, nvidia-tensorrt\n",
            "Successfully installed nvidia-cuda-runtime-cu13-0.0.0a0 nvidia-tensorrt-99.0.0 tensorrt-10.13.2.6 tensorrt_cu13-10.13.2.6 tensorrt_cu13_bindings-10.13.2.6 tensorrt_cu13_libs-10.13.2.6\n",
            "Collecting torch-tensorrt\n",
            "  Downloading torch_tensorrt-2.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_34_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=23 in /usr/local/lib/python3.12/dist-packages (from torch-tensorrt) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from torch-tensorrt) (4.15.0)\n",
            "Collecting dllist (from torch-tensorrt)\n",
            "  Downloading dllist-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: torch<2.9.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-tensorrt) (2.8.0+cu126)\n",
            "Collecting tensorrt<10.13.0,>=10.12.0 (from torch-tensorrt)\n",
            "  Downloading tensorrt-10.12.0.36.tar.gz (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt-cu12-bindings<10.13.0,>=10.12.0 (from torch-tensorrt)\n",
            "  Downloading tensorrt_cu12_bindings-10.12.0.36-cp312-none-manylinux_2_28_x86_64.whl.metadata (607 bytes)\n",
            "Collecting tensorrt-cu12-libs<10.13.0,>=10.12.0 (from torch-tensorrt)\n",
            "  Downloading tensorrt_cu12_libs-10.12.0.36.tar.gz (709 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-tensorrt) (2.0.2)\n",
            "Collecting tensorrt_cu12==10.12.0.36 (from tensorrt<10.13.0,>=10.12.0->torch-tensorrt)\n",
            "  Downloading tensorrt_cu12-10.12.0.36.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.12/dist-packages (from tensorrt-cu12-libs<10.13.0,>=10.12.0->torch-tensorrt) (12.6.77)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.9.0,>=2.8.0->torch-tensorrt) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.9.0,>=2.8.0->torch-tensorrt) (3.0.2)\n",
            "Downloading torch_tensorrt-2.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_34_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorrt_cu12_bindings-10.12.0.36-cp312-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dllist-2.0.0-py3-none-any.whl (5.7 kB)\n",
            "Building wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt-cu12-libs\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.12.0.36-py2.py3-none-any.whl size=46638 sha256=5ec917ca6f6bf4ede1815524e8fe64a4ba53aa364422f58f15bb4b986d2e0c25\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/d5/2f/cc5e3e56d49c61a02a7a8313f37db27d9af00e7f3463ed33e7\n",
            "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.12.0.36-py2.py3-none-any.whl size=17480 sha256=80f9eac2839e06802be7367adcf98aa9a1c1b7fa45dee0c57042a7aada46c81a\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/f7/e4/64a0965dcc74c067cee07482ef03cc18add0def626fb0ebc1c\n",
            "  Building wheel for tensorrt-cu12-libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt-cu12-libs: filename=tensorrt_cu12_libs-10.12.0.36-py2.py3-none-manylinux_2_28_x86_64.whl size=3095483544 sha256=3910039e1d49de0edfdc8bf273e40ad4b85a9d57c7c383fe0e22f75417df9610\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/8f/cb/35c0a5f8d17ece1927e42f2c49108ec506fd79b8e00490d416\n",
            "Successfully built tensorrt tensorrt_cu12 tensorrt-cu12-libs\n",
            "Installing collected packages: tensorrt-cu12-bindings, tensorrt-cu12-libs, dllist, tensorrt_cu12, tensorrt, torch-tensorrt\n",
            "  Attempting uninstall: tensorrt\n",
            "    Found existing installation: tensorrt 10.13.2.6\n",
            "    Uninstalling tensorrt-10.13.2.6:\n",
            "      Successfully uninstalled tensorrt-10.13.2.6\n",
            "Successfully installed dllist-2.0.0 tensorrt-10.12.0.36 tensorrt-cu12-bindings-10.12.0.36 tensorrt-cu12-libs-10.12.0.36 tensorrt_cu12-10.12.0.36 torch-tensorrt-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"torch.cuda.available:\", torch.cuda.is_available())\n",
        "print(\"torch.version.cuda:\", torch.version.cuda)\n",
        "print(\"device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"no-gpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1GrUVLe2OSD",
        "outputId": "07f9dde8-5457-449f-f16f-6040a1a40486"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.available: True\n",
            "torch.version.cuda: 12.6\n",
            "device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why: TensorRT requires an NVIDIA GPU + matching CUDA/driver. Check these first so you can pick the right binary/installation route. [NVIDIA Docs](https://docs.nvidia.com/deeplearning/tensorrt/latest/installing-tensorrt/overview.html?utm_source=chatgpt.com)"
      ],
      "metadata": {
        "id": "dP4SYBlO2dAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning and Understand"
      ],
      "metadata": {
        "id": "eo3Eu0WbakX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Workflow:**\n",
        "\n",
        "**Start from NVIDIA TensorRT Developer Guide**\n",
        "✔ The content references official NVIDIA docs and provides direct links for deeper reading (e.g., \\[Quick Start Guide], \\[Python API Docs]).\n",
        "\n",
        "---\n",
        "\n",
        "### **Practical steps:**\n",
        "\n",
        "**1. Train a model in PyTorch/TensorFlow**\n",
        "✔ This content trains a simple NN model for demontration purposes\n",
        "✔ It explains how to export the model to ONNX.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Export to ONNX**\n",
        "✔ Covered in detail with `torch.onnx.export()` including:\n",
        "\n",
        "* `opset_version`\n",
        "* `dynamic_axes`\n",
        "* Best practices like constant folding\n",
        "\n",
        "---\n",
        "\n",
        "**3. Use TensorRT to convert ONNX → TRT Engine**\n",
        "✔ The content gives\n",
        "* **Python API** (more programmatic and customizable)\n",
        "\n",
        "It also adds:\n",
        "\n",
        "* Workspace size setting\n",
        "* FP16 flag\n",
        "* Handling ONNX parse errors\n",
        "* Explanation of explicit batch flag\n",
        "\n",
        "---\n",
        "\n",
        "**4. Apply FP16 or INT8 quantization**\n",
        "✔ FP16: clearly shown via `config.set_flag(trt.BuilderFlag.FP16)`\n",
        "✔ INT8: mentions calibrators and links to references for implementation (not shown fully, but that's expected for a quick guide).\n",
        "\n",
        "---\n",
        "\n",
        "**5. Benchmark performance**\n",
        "✔ `trtexec --iterations` for quick profiling and latency measurement explained in detail.\n",
        "✔ Mentions Nsight tools for advanced profiling.\n"
      ],
      "metadata": {
        "id": "DnXq6Lqjbry1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a (Sample) Model in PyTorch"
      ],
      "metadata": {
        "id": "SoK93rhldRtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# 1. Set device\n",
        "# ----------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ----------------------\n",
        "# 2. Define transforms and load CIFAR-10\n",
        "# ----------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match dummy input shape\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ----------------------\n",
        "# 3. Define a simple CNN\n",
        "# ----------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 56 * 56, 128),  # 224→112→56 after two maxpools\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# ----------------------\n",
        "# 4. Loss and optimizer\n",
        "# ----------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ----------------------\n",
        "# 5. Training loop (short for demo)\n",
        "# ----------------------\n",
        "epochs = 2  # keep small for Colab demo\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ----------------------\n",
        "# 6. Test accuracy (quick check)\n",
        "# ----------------------\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# ----------------------\n",
        "# 7. Save model\n",
        "# ----------------------\n",
        "torch.save(model, '/content/model.pth')\n",
        "print(\"Model saved as /content/model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV2t0Dsa61iY",
        "outputId": "a4834d7a-2858-48ce-9058-7aca4afac780"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 42.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Loss: 1.6389\n",
            "Epoch [2/2], Loss: 1.3562\n",
            "Test Accuracy: 53.85%\n",
            "Model saved as /content/model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export to ONNX"
      ],
      "metadata": {
        "id": "3uw4uLJ2dajR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only weights\n",
        "torch.save(model.state_dict(), '/content/model.pth')\n",
        "\n",
        "# Load weights into model\n",
        "model = SimpleCNN()  # initialize same architecture\n",
        "model.load_state_dict(torch.load('/content/model.pth', map_location='cpu'))\n",
        "model.eval()\n",
        "\n",
        "# Dummy input (match the training shape)\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"model.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=13,\n",
        "    do_constant_folding=True,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
        ")\n",
        "\n",
        "print(\"ONNX model exported as model.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNFWyR7h7f-I",
        "outputId": "9a5b7823-52fb-462e-d238-f11fa872494d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3150253809.py:12: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model exported as model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "- Pick opset_version >= 11 (TensorRT supports modern opsets better).\n",
        "[PyTorch Documentation](https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html?utm_source=chatgpt.com)\n",
        "\n",
        "- Also, make sure that the saved model is in the `.state_dict()` format, otherwise it would have saved the entire model class object, which would pose hinderances when exporting to ONNX"
      ],
      "metadata": {
        "id": "UfO30lfB8kpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `torch.onnx.export(...)`**\n",
        "\n",
        "This is the main ONNX export function. Parameters:\n",
        "\n",
        "#### **`model`**\n",
        "\n",
        "* The PyTorch model you want to export.\n",
        "\n",
        "#### **`dummy_input`**\n",
        "\n",
        "* A sample input tensor to trace the model graph.\n",
        "\n",
        "#### **`\"model.onnx\"`**\n",
        "\n",
        "* Output ONNX file name.\n",
        "\n",
        "#### **`export_params=True`**\n",
        "\n",
        "* Exports **all trained parameters** along with the graph (weights, biases).\n",
        "\n",
        "#### **`opset_version=13`**\n",
        "\n",
        "* ONNX **operator set version**.\n",
        "* `13` is stable and widely supported by TensorRT.\n",
        "\n",
        "  * If your model uses newer ops, you might need a higher opset.\n",
        "\n",
        "(**Additional details:** Opset (Operator Set) in ONNX refers to the versioned set of operations (ops) that define the computation graph.\n",
        "\n",
        "Each ONNX model specifies an opset version (e.g., 11, 13, 17).\n",
        "\n",
        "Higher opset = newer operators and features.\n",
        "\n",
        "Exporting with a supported opset ensures compatibility with inference engines (like TensorRT).\n",
        "\n",
        "So, opset_version=13 means the model uses ONNX opset v13, which TensorRT supports.)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **`do_constant_folding=True`**\n",
        "\n",
        "* Performs **constant folding** during export:\n",
        "\n",
        "  * Any constant operations (e.g., adding 0, multiplying by 1) are simplified for efficiency.\n",
        "\n",
        "#### **`input_names=[\"input\"], output_names=[\"output\"]`**\n",
        "\n",
        "* Names for model input and output nodes (useful for later inference).\n",
        "\n",
        "#### **`dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}`**\n",
        "\n",
        "* Allows **dynamic batch size**.\n",
        "* Means:\n",
        "\n",
        "  * For the input tensor: axis `0` is named `batch_size` and can change at runtime.\n",
        "  * For the output tensor: axis `0` can also change.\n",
        "* Without this, the exported model will **only accept batch size 1**."
      ],
      "metadata": {
        "id": "mqTHG0lwTfxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxsim import simplify\n",
        "\n",
        "onnx_model = onnx.load(\"model.onnx\")\n",
        "model_simp, check = simplify(onnx_model)\n",
        "if not check:\n",
        "    raise RuntimeError(\"ONNX simplifier failed\")\n",
        "onnx.save(model_simp, \"model_simplified.onnx\")\n",
        "\n",
        "# sanity check\n",
        "onnx.checker.check_model(\"model_simplified.onnx\")\n",
        "print(\"ONNX simplified and valid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HjdVTG7UBXM",
        "outputId": "9b3ceec1-cff8-4625-e34d-2389fa2b4afa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX simplified and valid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Some framework exports add redundant nodes — simplify & check:\n",
        "- This reduces parsing errors with TensorRT and can speed up conversion."
      ],
      "metadata": {
        "id": "JnzYf9EkUCX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ONNX to TRT Engine"
      ],
      "metadata": {
        "id": "ji-chR67diWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Also Applies) FP16 Quantization"
      ],
      "metadata": {
        "id": "2zVMqe0mdrgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "builder = trt.Builder(TRT_LOGGER)\n",
        "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
        "\n",
        "with open(\"model_simplified.onnx\", \"rb\") as f:\n",
        "    if not parser.parse(f.read()):\n",
        "        for i in range(parser.num_errors):\n",
        "            print(parser.get_error(i))\n",
        "        raise RuntimeError(\"Failed to parse ONNX\")\n",
        "\n",
        "config = builder.create_builder_config()\n",
        "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1 GB\n",
        "config.set_flag(trt.BuilderFlag.FP16)\n",
        "\n",
        "# Add optimization profile for dynamic input\n",
        "profile = builder.create_optimization_profile()\n",
        "profile.set_shape(\"input\", (1, 3, 224, 224), (4, 3, 224, 224), (8, 3, 224, 224))\n",
        "config.add_optimization_profile(profile)\n",
        "\n",
        "# Build serialized engine\n",
        "serialized_engine = builder.build_serialized_network(network, config)\n",
        "if serialized_engine is None:\n",
        "    raise RuntimeError(\"Failed to build TensorRT engine\")\n",
        "\n",
        "with open(\"model_trt.engine\", \"wb\") as f:\n",
        "    f.write(serialized_engine)\n",
        "\n",
        "print(\"Saved TRT engine as model_trt.engine\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXYj0I74UYxE",
        "outputId": "8aa79522-1f7c-4530-b4b0-2294229dfe69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved TRT engine as model_trt.engine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the parser prints errors: inspect unsupported ops; either modify model, use ONNX Runtime, or implement a TRT plugin. [NVIDIA Docs](https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/python-api-docs.html?utm_source=chatgpt.com)"
      ],
      "metadata": {
        "id": "p-zBUb2CUZ87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`TRT_LOGGER = trt.Logger(trt.Logger.WARNING)`**\n",
        "  → Creates a logger for TensorRT messages (only warnings and errors will be shown).\n",
        "\n",
        "* **`builder = trt.Builder(TRT_LOGGER)`**\n",
        "  → Builder object responsible for compiling and optimizing the network.\n",
        "\n",
        "* **`builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))`**\n",
        "  → Creates a network definition with **explicit batch flag**, needed for dynamic shapes and optimization profiles.\n",
        "\n",
        "* **`parser = trt.OnnxParser(network, TRT_LOGGER)`**\n",
        "  → Parses the ONNX model into TensorRT’s internal network format.\n",
        "\n",
        "* **ONNX Parsing Block**\n",
        "\n",
        "  ```python\n",
        "  with open(\"model_simplified.onnx\", \"rb\") as f:\n",
        "      if not parser.parse(f.read()):\n",
        "          for i in range(parser.num_errors):\n",
        "              print(parser.get_error(i))\n",
        "          raise RuntimeError(\"Failed to parse ONNX\")\n",
        "  ```\n",
        "\n",
        "  → Reads ONNX file, parses it, and prints detailed errors if parsing fails.\n",
        "\n",
        "* **`config = builder.create_builder_config()`**\n",
        "  → Creates a configuration object for optimization and precision settings.\n",
        "\n",
        "* **`config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)`**\n",
        "  → Sets maximum temporary GPU memory (workspace) to **1 GB** for kernel selection and optimization.\n",
        "\n",
        "* **`config.set_flag(trt.BuilderFlag.FP16)`**\n",
        "  → Enables **FP16 precision mode**, which uses Tensor Cores for faster inference on GPUs that support FP16.\n",
        "\n",
        "* **Optimization Profile**\n",
        "\n",
        "  ```python\n",
        "  profile = builder.create_optimization_profile()\n",
        "  profile.set_shape(\"input\", (1, 3, 224, 224), (4, 3, 224, 224), (8, 3, 224, 224))\n",
        "  config.add_optimization_profile(profile)\n",
        "  ```\n",
        "\n",
        "  → Adds **dynamic shape support** for the input tensor named `\"input\"`:\n",
        "\n",
        "  * **min shape**: `(1, 3, 224, 224)`\n",
        "  * **opt shape**: `(4, 3, 224, 224)` (preferred batch size for optimization)\n",
        "  * **max shape**: `(8, 3, 224, 224)`\n",
        "\n",
        "  This was **not in your old code**—it enables **dynamic batching**, which is key for real deployments.\n",
        "\n",
        "* **`builder.build_serialized_network(network, config)`**\n",
        "  → Builds and **serializes the TensorRT engine** in one step (modern approach).\n",
        "  Older method `builder.build_engine()` is replaced with `build_serialized_network()` for efficiency.\n",
        "\n",
        "* **Write Engine to Disk**\n",
        "\n",
        "  ```python\n",
        "  with open(\"model_trt.engine\", \"wb\") as f:\n",
        "      f.write(serialized_engine)\n",
        "  ```\n",
        "\n",
        "  → Saves the engine as `model_trt.engine` for deployment.\n"
      ],
      "metadata": {
        "id": "buj3dcMWghdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "\n",
        "# ----------------------\n",
        "# LOAD ENGINE AND CONTEXT\n",
        "# ----------------------\n",
        "with open(\"/content/model_trt.engine\", \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
        "    engine = runtime.deserialize_cuda_engine(f.read())\n",
        "\n",
        "context = engine.create_execution_context()\n",
        "\n",
        "print(\"Engine object:\", engine)\n",
        "print(\"Engine is None?\", engine is None)\n",
        "\n",
        "try:\n",
        "    print(\"First binding name:\", engine.get_binding_name(0))\n",
        "except Exception as e:\n",
        "    print(\"Error calling get_binding_name(0):\", e)\n",
        "\n",
        "print(dir(engine))\n",
        "\n",
        "# ----------------------\n",
        "# Robust helper (no engine.num_bindings)\n",
        "# ----------------------\n",
        "\n",
        "def _collect_binding_indices(engine):\n",
        "    \"\"\"\n",
        "    Return a list of valid binding indices by probing get_binding_name(idx)\n",
        "    until it raises an exception. This avoids relying on engine.num_bindings\n",
        "    which may be missing in some TRT Python builds.\n",
        "    \"\"\"\n",
        "    idx = 0\n",
        "    indices = []\n",
        "    while True:\n",
        "        try:\n",
        "            _ = engine.get_binding_name(idx)\n",
        "            indices.append(idx)\n",
        "            idx += 1\n",
        "        except Exception:\n",
        "            break\n",
        "    return indices\n",
        "\n",
        "def allocate_buffers(engine, context, input_shapes):\n",
        "    \"\"\"\n",
        "    Allocate host/device buffers using TensorRT 10's tensor name-based API.\n",
        "    input_shapes: dict mapping input tensor name -> shape tuple\n",
        "    \"\"\"\n",
        "    inputs, outputs, bindings = [], [], []\n",
        "    stream = cuda.Stream()\n",
        "\n",
        "    # Get all tensor names\n",
        "    tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
        "\n",
        "    # Set dynamic shapes if provided\n",
        "    for name, shape in input_shapes.items():\n",
        "        if name not in tensor_names:\n",
        "            raise ValueError(f\"Input tensor '{name}' not found in engine tensors: {tensor_names}\")\n",
        "        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n",
        "            context.set_input_shape(name, tuple(shape))\n",
        "\n",
        "    # Allocate memory for all tensors\n",
        "    for name in tensor_names:\n",
        "        shape = context.get_tensor_shape(name)\n",
        "        if not shape:\n",
        "            raise RuntimeError(f\"Tensor '{name}' shape is None. Check optimization profile.\")\n",
        "        size = int(trt.volume(shape))\n",
        "        dtype = trt.nptype(engine.get_tensor_dtype(name))\n",
        "\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
        "\n",
        "        bindings.append(int(device_mem))\n",
        "\n",
        "        entry = {\"name\": name, \"host\": host_mem, \"device\": device_mem, \"shape\": tuple(shape)}\n",
        "        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n",
        "            inputs.append(entry)\n",
        "        else:\n",
        "            outputs.append(entry)\n",
        "\n",
        "    return inputs, outputs, bindings, stream\n",
        "\n",
        "def infer(context, bindings, inputs, outputs, stream):\n",
        "    # Copy input host → device\n",
        "    for inp in inputs:\n",
        "        cuda.memcpy_htod_async(inp[\"device\"], inp[\"host\"], stream)\n",
        "\n",
        "    # Run inference\n",
        "    context.execute_v2(bindings)\n",
        "\n",
        "    # Copy output device → host\n",
        "    for out in outputs:\n",
        "        cuda.memcpy_dtoh_async(out[\"host\"], out[\"device\"], stream)\n",
        "\n",
        "    stream.synchronize()\n",
        "\n",
        "    # Convert outputs to numpy\n",
        "    return [np.array(out[\"host\"]).reshape(out[\"shape\"]) for out in outputs]\n",
        "\n",
        "# ----------------------\n",
        "# EXAMPLE USAGE\n",
        "# ----------------------\n",
        "\n",
        "# Example: dynamic batch size 4 for input named exactly as the ONNX input (check name)\n",
        "dynamic_shape = {\"input\": (4, 3, 224, 224)}\n",
        "\n",
        "# Allocate\n",
        "inputs, outputs, bindings, stream = allocate_buffers(engine, context, dynamic_shape)\n",
        "\n",
        "# Fill input (host buffer is 1D pagelocked array)\n",
        "in_shape = inputs[0][\"shape\"]\n",
        "dummy_input = np.random.randn(*in_shape).astype(np.float32).ravel()\n",
        "np.copyto(inputs[0][\"host\"], dummy_input)\n",
        "\n",
        "# Run\n",
        "results = infer(context, bindings, inputs, outputs, stream)\n",
        "\n",
        "print(\"Output binding shapes:\", [out[\"shape\"] for out in outputs])\n",
        "print(\"First 10 output values:\", results[0].flatten()[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lyRXMfxUvJQ",
        "outputId": "6ef0bd82-62e9-42b2-eca5-64ea0b02a962"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Engine object: <tensorrt_bindings.tensorrt.ICudaEngine object at 0x7f00d838d530>\n",
            "Engine is None? False\n",
            "Error calling get_binding_name(0): 'tensorrt_bindings.tensorrt.ICudaEngine' object has no attribute 'get_binding_name'\n",
            "['__class__', '__del__', '__delattr__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pybind11_module_local_v4_gcc_libstdcpp_cxxabi1016__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'create_engine_inspector', 'create_execution_context', 'create_execution_context_without_device_memory', 'create_runtime_config', 'create_serialization_config', 'device_memory_size', 'device_memory_size_v2', 'engine_capability', 'error_recorder', 'get_device_memory_size_for_profile', 'get_device_memory_size_for_profile_v2', 'get_tensor_bytes_per_component', 'get_tensor_components_per_element', 'get_tensor_dtype', 'get_tensor_format', 'get_tensor_format_desc', 'get_tensor_location', 'get_tensor_mode', 'get_tensor_name', 'get_tensor_profile_shape', 'get_tensor_profile_values', 'get_tensor_shape', 'get_tensor_vectorized_dim', 'get_weight_streaming_automatic_budget', 'hardware_compatibility_level', 'has_implicit_batch_dimension', 'is_debug_tensor', 'is_shape_inference_io', 'minimum_weight_streaming_budget', 'name', 'num_aux_streams', 'num_io_tensors', 'num_layers', 'num_optimization_profiles', 'profiling_verbosity', 'refittable', 'serialize', 'serialize_with_config', 'streamable_weights_size', 'tactic_sources', 'weight_streaming_budget', 'weight_streaming_budget_v2', 'weight_streaming_scratch_memory_size']\n",
            "Output binding shapes: [(4, 10)]\n",
            "First 10 output values: [  9.4375      8.0703125   3.0234375  -8.3125     -4.7578125 -10.34375\n",
            " -10.0390625 -10.6484375  -2.7226562   7.7226562]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TensorRT Inference Pipeline with PyCUDA (Dynamic Shapes)**\n",
        "\n",
        "This script demonstrates how to **load a TensorRT engine**, allocate memory for inputs/outputs, and run inference using **TensorRT 10 API** and **PyCUDA**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Setup and Imports**\n",
        "\n",
        "```python\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import numpy as np\n",
        "```\n",
        "\n",
        "* **tensorrt** → Core library for TensorRT engine handling.\n",
        "* **pycuda.driver** → CUDA memory allocation and data transfer.\n",
        "* **pycuda.autoinit** → Automatically initializes CUDA context.\n",
        "* **numpy** → Handles input/output tensors.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Load Serialized TensorRT Engine**\n",
        "\n",
        "```python\n",
        "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
        "\n",
        "with open(\"/content/model_trt.engine\", \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
        "    engine = runtime.deserialize_cuda_engine(f.read())\n",
        "\n",
        "context = engine.create_execution_context()\n",
        "```\n",
        "\n",
        "* **TRT\\_LOGGER** → Logger for TensorRT warnings and errors.\n",
        "* **deserialize\\_cuda\\_engine** → Loads a serialized `.engine` file into memory.\n",
        "* **create\\_execution\\_context()** → Creates an execution context for inference (manages dynamic shapes, bindings).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Inspect Engine**\n",
        "\n",
        "```python\n",
        "print(\"Engine object:\", engine)\n",
        "print(\"Engine is None?\", engine is None)\n",
        "```\n",
        "\n",
        "* Verifies that the engine is successfully loaded.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Dynamic Shape & Buffer Management**\n",
        "\n",
        "TensorRT 10 uses **tensor names** instead of old binding APIs.\n",
        "\n",
        "### **Helper: Allocate Buffers**\n",
        "\n",
        "```python\n",
        "def allocate_buffers(engine, context, input_shapes):\n",
        "    inputs, outputs, bindings = [], [], []\n",
        "    stream = cuda.Stream()\n",
        "\n",
        "    tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
        "\n",
        "    # Set dynamic input shape\n",
        "    for name, shape in input_shapes.items():\n",
        "        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n",
        "            context.set_input_shape(name, tuple(shape))\n",
        "\n",
        "    # Allocate memory for each tensor\n",
        "    for name in tensor_names:\n",
        "        shape = context.get_tensor_shape(name)\n",
        "        size = int(trt.volume(shape))\n",
        "        dtype = trt.nptype(engine.get_tensor_dtype(name))\n",
        "\n",
        "        host_mem = cuda.pagelocked_empty(size, dtype)  # CPU memory\n",
        "        device_mem = cuda.mem_alloc(host_mem.nbytes)   # GPU memory\n",
        "\n",
        "        bindings.append(int(device_mem))\n",
        "\n",
        "        entry = {\"name\": name, \"host\": host_mem, \"device\": device_mem, \"shape\": tuple(shape)}\n",
        "        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n",
        "            inputs.append(entry)\n",
        "        else:\n",
        "            outputs.append(entry)\n",
        "\n",
        "    return inputs, outputs, bindings, stream\n",
        "```\n",
        "\n",
        "✅ **Key points:**\n",
        "\n",
        "* **`context.set_input_shape(name, shape)`** → Required for dynamic input sizes.\n",
        "* **Host memory (CPU)**: `cuda.pagelocked_empty()` → Pinned memory for fast transfer.\n",
        "* **Device memory (GPU)**: `cuda.mem_alloc()` → Allocates space on GPU.\n",
        "* **bindings\\[]** → List of device memory pointers passed to TensorRT.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Inference Execution**\n",
        "\n",
        "```python\n",
        "def infer(context, bindings, inputs, outputs, stream):\n",
        "    for inp in inputs:\n",
        "        cuda.memcpy_htod_async(inp[\"device\"], inp[\"host\"], stream)\n",
        "\n",
        "    context.execute_v2(bindings)\n",
        "\n",
        "    for out in outputs:\n",
        "        cuda.memcpy_dtoh_async(out[\"host\"], out[\"device\"], stream)\n",
        "\n",
        "    stream.synchronize()\n",
        "\n",
        "    return [np.array(out[\"host\"]).reshape(out[\"shape\"]) for out in outputs]\n",
        "```\n",
        "\n",
        "✅ **Steps:**\n",
        "\n",
        "* Copy **inputs from host → GPU** (`memcpy_htod_async`).\n",
        "* Execute inference with `context.execute_v2(bindings)`.\n",
        "* Copy **outputs from GPU → host** (`memcpy_dtoh_async`).\n",
        "* Synchronize CUDA stream to ensure completion.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Example Usage**\n",
        "\n",
        "```python\n",
        "dynamic_shape = {\"input\": (4, 3, 224, 224)}\n",
        "\n",
        "# Allocate memory\n",
        "inputs, outputs, bindings, stream = allocate_buffers(engine, context, dynamic_shape)\n",
        "\n",
        "# Fill input tensor\n",
        "dummy_input = np.random.randn(*inputs[0][\"shape\"]).astype(np.float32).ravel()\n",
        "np.copyto(inputs[0][\"host\"], dummy_input)\n",
        "\n",
        "# Run inference\n",
        "results = infer(context, bindings, inputs, outputs, stream)\n",
        "\n",
        "print(\"Output binding shapes:\", [out[\"shape\"] for out in outputs])\n",
        "print(\"First 10 output values:\", results[0].flatten()[:10])\n",
        "```\n",
        "\n",
        "✅ **What happens here:**\n",
        "\n",
        "* **Dynamic batch size**: `(4, 3, 224, 224)`.\n",
        "* Generate random input data and copy into host buffer.\n",
        "* Run inference and display output shape & sample predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **⚠ Why `get_binding_name()` failed?**\n",
        "\n",
        "* TensorRT 10 uses **tensor name-based API**:\n",
        "\n",
        "  * Use `engine.get_tensor_name(i)` instead of `get_binding_name()`.\n",
        "  * Use `engine.num_io_tensors` instead of `num_bindings`."
      ],
      "metadata": {
        "id": "yZv3Tt2og8h-"
      }
    }
  ]
}